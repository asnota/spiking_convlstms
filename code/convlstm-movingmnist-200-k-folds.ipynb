{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nimport torchvision.transforms as T\nfrom sklearn.model_selection import KFold\nimport os\nimport copy\nimport matplotlib.pyplot as plt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install codecarbon comet_ml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from comet_ml import Experiment\nfrom codecarbon import EmissionsTracker\nfrom datetime import datetime\n\n# Initialise and start CodeCarbon tracker\ntracker = EmissionsTracker()\ntracker.start()\n\nstart_time = datetime.now()\nprint(f'Start time is {start_time}')\n\n# Initialise the Comet experiment\nexperiment = Experiment(\n    api_key=\"XXXXXXXXXXXXXXXXXXXXXXXXX\",\n    project_name=\"\",\n    workspace=\"\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Data\n! wget -q https://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy\n\n# Load test partition\nMovingMNIST = np.load('mnist_test_seq.npy').transpose(1, 0, 2, 3)\n\n# Shuffle Data\nnp.random.shuffle(MovingMNIST)\n\n# Get 200 entries only\nMovingMNIST_reduced = MovingMNIST[:216]\n\n#Resize 64x64 images to 28x28\ntf = T.Compose([\n     T.ToPILImage(),\n     T.Resize((28)),\n     T.ToTensor() # Returns a tensor with normalized values between 0 and 1\n])\n\nresized_seqs = []\nfor seq in MovingMNIST_reduced:\n    resized_imgs = []\n    for img in seq[:5]:\n        resized_img = tf(img)\n        resized_imgs.append(resized_img)\n    resized_imgs_stack = torch.stack(resized_imgs)\n    resized_seqs.append(resized_imgs_stack)\n\nresized_seqs_stack = torch.stack(resized_seqs)\nresized_seqs_reshaped = resized_seqs_stack.reshape(216, 1, 5, 28, 28)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Utils\ndef collate(batch):\n    batch = torch.stack(batch)\n    # batch = batch / 255.0 # Double normalization, cause ToTensor transform already normalized the values\n    batch = batch.to(device)    \n    return batch[:,:,0:4], batch[:,:,4]\n\ndef reset_weights(m):\n  '''\n    Try resetting model weights to avoid\n    weight leakage.\n  '''\n  for layer in m.children():\n        if hasattr(layer, 'reset_parameters'):\n            print(f'Reset trainable parameters of layer = {layer}')\n            layer.reset_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(resized_seqs_reshaped, batch_size=15, collate_fn=collate, drop_last=True)\ndata, target = next(iter(train_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\nfig = plt.figure(figsize=(4, 4))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(1, 4),  # creates 2x2 grid of axes\n                 axes_pad=0.05,  # pad between axes in inch.\n                 )\n\nfor ax, im in zip(grid, data[0][0]):\n    # Iterating over the grid returns the Axes.\n    ax.imshow(im)\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ConvLSTM cell and layer\n# Original ConvLSTM cell as proposed by Shi et al.\nclass ConvLSTMCell(nn.Module):\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n        super(ConvLSTMCell, self).__init__()\n        if activation == \"tanh\":\n            self.activation = torch.tanh \n        elif activation == \"relu\":\n            self.activation = torch.relu\n        \n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        self.conv = nn.Conv2d(\n            in_channels=in_channels + out_channels, \n            out_channels=4 * out_channels, \n            kernel_size=kernel_size, \n            padding=padding)           \n\n        # Initialize weights for Hadamard Products\n        self.W_ci = nn.Parameter(torch.rand(out_channels, *frame_size)) # out-channels=28\n        self.W_co = nn.Parameter(torch.rand(out_channels, *frame_size)) # frame_size=(28, 28)\n        self.W_cf = nn.Parameter(torch.rand(out_channels, *frame_size))\n\n    def forward(self, X, H_prev, C_prev):\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n\n        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n\n        # Current Cell output\n        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n\n        # Current Hidden State\n        H = output_gate * self.activation(C)\n        return H, C\n\n### ConvLSTM layer\nclass ConvLSTM(nn.Module):\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n\n        super(ConvLSTM, self).__init__()\n        self.out_channels = out_channels\n\n        # We will unroll this over time steps\n        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n        kernel_size, padding, activation, frame_size)\n\n    def forward(self, X):\n        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n        # Get the dimensions\n        batch_size, _, seq_len, height, width = X.size()\n\n        # Initialize output\n        output = torch.zeros(batch_size, self.out_channels, seq_len, \n        height, width, device=device)\n        \n        # Initialize Hidden State\n        H = torch.zeros(batch_size, self.out_channels, \n        height, width, device=device)\n\n        # Initialize Cell Input\n        C = torch.zeros(batch_size,self.out_channels, \n        height, width, device=device)\n\n        # Unroll over time steps\n        for time_step in range(seq_len):\n            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n            output[:,:,time_step] = H\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ConvLSTM model with 1 convLSTM layer\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n    activation, frame_size):\n        super(Seq2Seq, self).__init__()\n        self.sequential = nn.Sequential()\n\n        # Add First layer (Different in_channels than the rest)\n        self.sequential.add_module(\n            \"convlstm1\", ConvLSTM(\n                in_channels=num_channels, out_channels=num_kernels,\n                kernel_size=kernel_size, padding=padding, \n                activation=activation, frame_size=frame_size)\n        )\n\n        self.sequential.add_module(\n            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n        ) \n\n        # Add Convolutional Layer to predict output frame\n        self.conv = nn.Conv2d(\n            in_channels=num_kernels, out_channels=num_channels,\n            kernel_size=kernel_size, padding=padding)\n\n    def forward(self, X):\n        # Forward propagation through all the layers\n        output = self.sequential(X)\n\n        # Return only the last output frame\n        output = self.conv(output[:,:,-1])        \n        return nn.Sigmoid()(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### K-fold Cross Validator\n# Params\ntorch.manual_seed(42)\nnum_epochs = 100\ncriterion = nn.BCELoss(reduction='sum')\n# ToDo\n# Try mean square error loss\n\n# Fold results storage objects\ntrain_start_results = {}\nval_start_results = {}\n\ntrain_end_results = {}\nval_end_results = {}\n\n# Per fold epoch results storage objects\ntrain_results_per_epoch = []\nval_results_per_epoch = []\n\ntrain_results = []\nval_results = []\n\n# Define the K-fold Cross Validator\nk_folds = 5\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Whole dataset\ndataset = resized_seqs_reshaped\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n    print(f'FOLD {fold}')\n    \n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    val_subsampler = SubsetRandomSampler(val_ids)\n    \n    # Define data loaders for training and testing data in this fold\n    train_loader = DataLoader(dataset, batch_size=15, collate_fn=collate, sampler=train_subsampler)\n    val_loader = DataLoader(dataset, batch_size=15, collate_fn=collate, sampler=val_subsampler)\n    \n    # Initialization\n    model = Seq2Seq(num_channels=1, num_kernels=28, kernel_size=(3, 3), padding=(1, 1), activation=\"tanh\", frame_size=(28, 28)).to(device)\n    model.apply(reset_weights)  \n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    \n    train_results_per_epoch = []\n    val_results_per_epoch = []\n    for epoch in range(1, num_epochs+1):\n        train_loss = 0                                                 \n        model.train()        \n        for batch, (x, y) in enumerate(train_loader, 1):  \n            output = model(x)\n            \n            loss_train = criterion(output.flatten(), y.flatten())       \n            loss_train.backward() \n            \n            optimizer.step()                                               \n            optimizer.zero_grad() \n                                      \n            train_loss += loss_train.item()                                 \n        total_train_loss = train_loss / len(train_loader.dataset)\n        train_results_per_epoch.append(total_train_loss)\n        \n        val_loss = 0                                                 \n        model.eval()                                                   \n        with torch.no_grad():                                          \n            for x, y in val_loader:                          \n                output = model(x)                                   \n                loss_val = criterion(output.flatten(), y.flatten())                \n                val_loss += loss_val.item()                                \n        total_val_loss = val_loss / len(val_loader.dataset)\n        val_results_per_epoch.append(total_val_loss)\n        \n        # Store scores        \n        if epoch == 1:\n            val_start_results[fold] = total_val_loss\n            train_start_results[fold] = total_train_loss\n        else:\n            val_end_results[fold] = total_val_loss\n            train_end_results[fold] = total_train_loss\n            \n        print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n            epoch, total_train_loss, total_val_loss))\n    train_results.append(train_results_per_epoch)\n    val_results.append(val_results_per_epoch)\n    \n    # Saving the model\n    save_path = f'./convlstm-movingmnist200-model-fold-{fold}.pth'\n    torch.save(model.state_dict(), save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference\ndata_loader = DataLoader(dataset, batch_size=1, collate_fn=collate, drop_last=True)\ndata, target = next(iter(data_loader))\n\nmodel.eval()                                                   \nwith torch.no_grad():                                          \n    output = model(data)\noutput.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generated image visualization\nimg_gen = output[0].reshape(28, 28, 1)\nplt.imshow(img_gen)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target image visualization\ntarget_reshaped = target[0].reshape(28, 28, 1)\nplt.imshow(target_reshaped)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference for 15 sequences\ndata_loader = DataLoader(dataset, batch_size=15, collate_fn=collate, drop_last=True)\ndata, target = next(iter(data_loader))\n\nmodel.eval()                                                   \nwith torch.no_grad():                                          \n    output = model(data)\n\n# Reshape targets and generated\ntargets = target.reshape(15, 28, 28, 1)\nimgs_gen = output.reshape(15, 28, 28, 1)\n\n# Join tensors for a singe image\ncombined = torch.cat((targets, imgs_gen), 0)\ncombined.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\nfig = plt.figure(figsize=(15, 15))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(2, 15),  # creates 2x2 grid of axes\n                 axes_pad=0.05,  # pad between axes in inch.\n                 )\n\nfor ax, im in zip(grid, combined):\n    # Iterating over the grid returns the Axes.\n    ax.imshow(im)\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print start fold results\nprint(f'Start K-FOLD RESULTS FOR {k_folds} FOLDS')\nsum = 0.0\nfor key, value in train_start_results.items():\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average train: {sum/len(train_start_results.items())}')\n\nsum = 0.0\nfor key, value in val_start_results.items():\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average val: {sum/len(val_start_results.items())}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print final fold results\nprint(f'End K-FOLD RESULTS FOR {k_folds} FOLDS')\nsum = 0.0\nfor key, value in train_end_results.items():é\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average train: {sum/len(train_end_results.items())}')\n\nsum = 0.0\nfor key, value in val_end_results.items():\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average val: {sum/len(val_end_results.items())}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and validation results\n\nimport matplotlib.pyplot as plt\nx = list(range(0, 100))\n\nfig, ax = plt.subplots()\nt1, = ax.plot(x, train_results[0], c=\"blue\")\nt2, = ax.plot(x, train_results[1], c=\"brown\")\nt3, = ax.plot(x, train_results[2], c=\"green\")\nt4, = ax.plot(x, train_results[3], c=\"orange\")\nt5, = ax.plot(x, train_results[4], c=\"magenta\")\nv1, = ax.plot(x, val_results[0], c=\"blue\", ls=\"dashed\")\nv2, = ax.plot(x, val_results[1], c=\"brown\", ls=\"dashed\")\nv3, = ax.plot(x, val_results[2], c=\"green\", ls=\"dashed\")\nv4, = ax.plot(x, val_results[3], c=\"orange\", ls=\"dashed\")\nv5, = ax.plot(x, val_results[4], c=\"magenta\", ls=\"dashed\")\nax.legend((t1, t2, t3, t4, t5, v1, v2, v3, v4, v5), ('1st train fold', '2nd train fold', \"3rd train fold\", \"4th train fold\", \"5th train fold\", '1st val fold', '2nd val fold', \"3rd val fold\", \"4th val fold\", \"5th val fold\"), loc='upper right', shadow=True)\nax.set_xlabel('epochs')\nax.set_ylabel('loss')\nax.set_title('Train and validation results for 5 folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\nfig, ax = plt.subplots()\nt1, = ax.plot(x, train_results[0], c=\"blue\")\nt2, = ax.plot(x, train_results[1], c=\"brown\")\nt3, = ax.plot(x, train_results[2], c=\"green\")\nt4, = ax.plot(x, train_results[3], c=\"orange\")\nt5, = ax.plot(x, train_results[4], c=\"magenta\")\nax.legend((t1, t2, t3, t4, t5), ('1st train fold', '2nd train fold', \"3rd train fold\", \"4th train fold\", \"5th train fold\"), loc='upper right', shadow=True)\nax.set_xlabel('epochs')\nax.set_ylabel('loss')\nax.set_title('Train results for 5 folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\nfig, ax = plt.subplots()\nv1, = ax.plot(x, val_results[0], c=\"blue\", ls=\"dashed\")\nv2, = ax.plot(x, val_results[1], c=\"brown\", ls=\"dashed\")\nv3, = ax.plot(x, val_results[2], c=\"green\", ls=\"dashed\")\nv4, = ax.plot(x, val_results[3], c=\"orange\", ls=\"dashed\")\nv5, = ax.plot(x, val_results[4], c=\"magenta\", ls=\"dashed\")\nax.legend((v1, v2, v3, v4, v5), (\"1st val fold\", \"2nd val fold\", \"3rd val fold\", \"4th val fold\", \"5th val fold\"), loc='upper right', shadow=True)\nax.set_xlabel('epochs')\nax.set_ylabel('loss')\nax.set_title('Validation results for 5 folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and val for 1 fold\nfig, ax = plt.subplots()\nt1, = ax.plot(x, train_results[0], c=\"blue\")\nv1, = ax.plot(x, val_results[0], c=\"blue\", ls=\"dashed\")\nax.legend((t1, v1), ('1st train fold', '1st val fold'), loc='upper right', shadow=True)\nax.set_xlabel('epochs')\nax.set_ylabel('loss')\nax.set_title('Train and validation results for 1 folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open(\"./\" + \"train_results.json\", 'w') as outfile:\n    json.dump(train_results, outfile)\nwith open(\"./\" + \"val_results.json\", 'w') as outfile:\n    json.dump(val_results, outfile)","metadata":{},"execution_count":null,"outputs":[]}]}
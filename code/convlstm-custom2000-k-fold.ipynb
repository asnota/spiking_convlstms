{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import KFold\nimport os\nimport copy\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCUDA_LAUNCH_BLOCKING=1\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install codecarbon comet_ml","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from comet_ml import Experiment\nfrom codecarbon import EmissionsTracker\nfrom datetime import datetime\n\n# Initialise and start CodeCarbon tracker\ntracker = EmissionsTracker()\ntracker.start()\n\nstart_time = datetime.now()\nprint(f'Start time is {start_time}')\n\n# Initialise the Comet experiment\nexperiment = Experiment(\n    api_key=\"XXXXXXXXXXXXXXXXXXXXXXXXX\",\n    project_name=\"\",\n    workspace=\"\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Custom data unsupervised 2D  starting from 1\n### Definition\n#### Chord dictionaries\ntriads_dictionary = {\n    5:\"I35\",\n    6:\"I6\",\n    7:\"I46\",\n    12:\"II35\",\n    13:\"II6\",\n    14:\"II46\",\n    19:\"III35\",\n    20:\"III6\",\n    21:\"III46\",\n    26:\"IV35\",\n    27:\"IV6\",\n    28:\"IV46\",\n    33:\"V35\",\n    34:\"V6\",\n    35:\"V46\",\n    40:\"VI35\",\n    41:\"VI6\",\n    42:\"VI46\",\n    47:\"VII35\",\n    48:\"VII6\",\n    49:\"VII46\"    \n}\nseventhchords_dictionary = {\n    1:\"I7\",\n    2:\"I56\",\n    3:\"I34\",\n    4:\"I2\",\n    8:\"II7\",\n    9:\"II56\",\n    10:\"II34\",\n    11:\"II2\",\n    15:\"III7\",\n    16:\"III56\",\n    17:\"III34\",\n    18:\"III2\",\n    22:\"IV7\",\n    23:\"IV56\",\n    24:\"IV34\",\n    25:\"IV2\",\n    29:\"V7\",\n    30:\"V56\",\n    31:\"V34\",\n    32:\"V2\",\n    36:\"VI7\",\n    37:\"VI56\",\n    38:\"VI34\",\n    39:\"VI2\",\n    43:\"VII7\",\n    44:\"VII56\",\n    45:\"VII34\",\n    46:\"VII2\",\n}\nchords_dictionary = {**seventhchords_dictionary, **triads_dictionary}\nchords_dictionary\n\n#### Sequences\nsequences = [\n                ['I7', 'II35', 'I56', 'IV6', 'V6'],\n                ['I7', 'VII6', 'I56', 'IV6', 'V6'],\n                ['I7', 'V46', 'I56', 'IV6', 'V6'],\n\n                ['I7', 'II35', 'I56', 'IV6', 'II35'],\n                ['I7', 'VII6', 'I56', 'IV6', 'II35'],\n                ['I7', 'V46', 'I56', 'IV6', 'II35'],\n\n                ['I7', 'II35', 'I56', 'IV2', 'II35'],\n                ['I7', 'VII6', 'I56', 'IV2', 'II35'],\n                ['I7', 'V46', 'I56', 'IV2', 'II35'],\n\n                ['I7', 'II35', 'VI34', 'II35', 'V6'],\n                ['I7', 'VII6', 'VI34', 'II35', 'V6'],\n                ['I7', 'V46', 'VI34', 'II35', 'V6'],\n\n                ['I7', 'II35', 'VI34', 'IV35', 'II6'],\n                ['I7', 'VII6', 'VI34', 'IV35', 'II6'],\n                ['I7', 'V46', 'VI34', 'IV35', 'II6'],\n\n                ['I7', 'II35', 'VI34', 'VII6', 'VI6'],\n                ['I7', 'VII6', 'VI34', 'VII6', 'VI6'],\n                ['I7', 'V46', 'VI34', 'VII6', 'VI6'],\n\n                ['I7', 'II35', 'VI34', 'IV2', 'V34'],\n                ['I7', 'VII6', 'VI34', 'IV2', 'V34'],\n                ['I7', 'V46', 'VI34', 'IV2', 'V34'],\n\n                ['I7', 'II35', 'IV2', 'VII6', 'VI6'],\n                ['I7', 'VII6', 'IV2', 'VII6', 'VI6'],\n                ['I7', 'V46', 'IV2', 'VII6', 'VI6'],\n\n                ['I7', 'II35', 'IV2', 'V34', 'III2'],\n                ['I7', 'VII6', 'IV2', 'V34', 'III2'],\n                ['I7', 'V46', 'IV2', 'V34', 'III2'],\n\n                ['I7', 'II35', 'IV2', 'VII56', 'V34'],\n                ['I7', 'VII6', 'IV2', 'VII56', 'V34'],\n                ['I7', 'V46', 'IV2', 'VII56', 'V34'],\n\n\n                ['II7', 'III35', 'II56', 'V35', 'I6'],\n                ['II7', 'I6', 'II56', 'V35', 'I6'],\n                ['II7', 'VI46', 'II56', 'V35', 'I6'],\n\n                ['II7', 'III35', 'II56', 'V6', 'VI35'],\n                ['II7', 'I6', 'II56', 'V6', 'VI35'],\n                ['II7', 'VI46', 'II56', 'V6', 'VI35'],\n\n                ['II7', 'III35', 'II56', 'V2', 'I6'],\n                ['II7', 'I6', 'II56', 'V2', 'I6'],\n                ['II7', 'VI46', 'II56', 'V2', 'I6'],\n\n                ['II7', 'III35', 'II56', 'V35', 'I35'],\n                ['II7', 'I6', 'II56', 'V35', 'I35'],\n                ['II7', 'VI46', 'II56', 'V35', 'I35'],\n\n                ['II7', 'III35', 'VII34', 'III35', 'VI6'],\n                ['II7', 'I6', 'VII34', 'III35', 'VI6'],\n                ['II7', 'VI46', 'VII34', 'III35', 'VI6'],\n\n                ['II7', 'III35', 'VII34', 'V35', 'III35'],\n                ['II7', 'I6', 'VII34', 'V35', 'III35'],\n                ['II7', 'VI46', 'VII34', 'V35', 'III35'],\n\n                ['II7', 'III35', 'VII34', 'I56', 'II6'],\n                ['II7', 'I6', 'VII34', 'I56', 'II6'],\n                ['II7', 'VI46', 'VII34', 'I56', 'II6'],\n\n                ['II7', 'III35', 'VII34', 'V2', 'VI34'],\n                ['II7', 'I6', 'VII34', 'V2', 'VI34'],\n                ['II7', 'VI46', 'VII34', 'V2', 'VI34'],\n\n                ['II7', 'III35', 'V2', 'I6', 'II35'],\n                ['II7', 'I6', 'V2', 'I6', 'II35'],\n                ['II7', 'VI46', 'V2', 'I6', 'II35'],\n\n                ['II7', 'III35', 'V2', 'VI34', 'IV2'],\n                ['II7', 'I6', 'V2', 'VI34', 'IV2'],\n                ['II7', 'VI46', 'V2', 'VI34', 'IV2'],\n\n                ['II7', 'III35', 'V2', 'I56', 'VI34'],\n                ['II7', 'I6', 'V2', 'I56', 'VI34'],\n                ['II7', 'VI46', 'V2', 'I56', 'VI34'],\n\n\n                ['III7', 'IV35', 'III56', 'VI35', 'II35'],\n                ['III7', 'II6', 'III56', 'VI35', 'II35'],\n                ['III7', 'VII46', 'III56', 'VI35', 'II35'],\n\n                ['III7', 'IV35', 'III56', 'VI35', 'IV6'],\n                ['III7', 'II6', 'III56', 'VI35', 'IV6'],\n                ['III7', 'VII46', 'III56', 'VI35', 'IV6'],\n\n                ['III7', 'IV35', 'III56', 'VI2', 'IV35'],\n                ['III7', 'II6', 'III56', 'VI2', 'IV35'],\n                ['III7', 'VII46', 'III56', 'VI2', 'IV35'],\n\n                ['III7', 'IV35', 'I34', 'IV35', 'V6'],\n                ['III7', 'II6', 'I34', 'IV35', 'V6'],\n                ['III7', 'VII46', 'I34', 'IV35', 'V6'],\n\n                ['III7', 'IV35', 'I34', 'IV35', 'I35'],\n                ['III7', 'II6', 'I34', 'IV35', 'I35'],\n                ['III7', 'VII46', 'I34', 'IV35', 'I35'],\n\n                ['III7', 'IV35', 'I34', 'VI35', 'IV6'],\n                ['III7', 'II6', 'I34', 'VI35', 'IV6'],\n                ['III7', 'VII46', 'I34', 'VI35', 'IV6'],\n\n                ['III7', 'IV35', 'I34', 'II6', 'I6'],\n                ['III7', 'II6', 'I34', 'II6', 'I6'],\n                ['III7', 'VII46', 'I34', 'II6', 'I6'],\n\n                ['III7', 'IV35', 'I34', 'VI2', 'II6'],\n                ['III7', 'II6', 'I34', 'VI2', 'II6'],\n                ['III7', 'VII46', 'I34', 'VI2', 'II6'],\n\n                ['III7', 'IV35', 'VI2', 'II6', 'I6'],\n                ['III7', 'II6', 'VI2', 'II6', 'I6'],\n                ['III7', 'VII46', 'VI2', 'II6', 'I6'],\n\n                ['III7', 'IV35', 'VI2', 'VII34', 'V2'],\n                ['III7', 'II6', 'VI2', 'VII34', 'V2'],\n                ['III7', 'VII46', 'VI2', 'VII34', 'V2'],\n\n                ['III7', 'IV35', 'VI2', 'II56', 'V2'],\n                ['III7', 'II6', 'VI2', 'II56', 'V2'],\n                ['III7', 'VII46', 'VI2', 'II56', 'V2'],\n\n\n                ['IV7', 'V35', 'IV56', 'II34', 'V6'],\n                ['IV7', 'III6', 'IV56', 'II34', 'V6'],\n                ['IV7', 'I46', 'IV56', 'II34', 'V6'],\n\n                ['IV7', 'V35', 'IV56', 'II34', 'III56'],\n                ['IV7', 'III6', 'IV56', 'II34', 'III56'],\n                ['IV7', 'I46', 'IV56', 'II34', 'III56'],\n\n                ['IV7', 'V35', 'IV56', 'VII2', 'III6'],\n                ['IV7', 'III6', 'IV56', 'VII2', 'III6'],\n                ['IV7', 'I46', 'IV56', 'VII2', 'III6'],\n\n                ['IV7', 'V35', 'II34', 'V35', 'I6'],\n                ['IV7', 'III6', 'II34', 'V35', 'I6'],\n                ['IV7', 'I46', 'II34', 'V35', 'I6'],\n\n                ['IV7', 'V35', 'II34', 'VII2', 'III6'],\n                ['IV7', 'III6', 'II34', 'VII2', 'III6'],\n                ['IV7', 'I46', 'II34', 'VII2', 'III6'],\n\n                ['IV7', 'V35', 'II34', 'III56', 'II6'],\n                ['IV7', 'III6', 'II34', 'III56', 'II6'],\n                ['IV7', 'I46', 'II34', 'III56', 'II6'],\n\n                ['IV7', 'V35', 'II34', 'VII2', 'I34'],\n                ['IV7', 'III6', 'II34', 'VII2', 'I34'],\n                ['IV7', 'I46', 'II34', 'VII2', 'I34'],\n\n                ['IV7', 'V35', 'VII2', 'I34', 'II56'],\n                ['IV7', 'III6', 'VII2', 'I34', 'II56'],\n                ['IV7', 'I46', 'VII2', 'I34', 'II56'],\n\n                ['IV7', 'V35', 'VII2', 'I34', 'VI2'],\n                ['IV7', 'III6', 'VII2', 'I34', 'VI2'],\n                ['IV7', 'I46', 'VII2', 'I34', 'VI2'],\n\n                ['IV7', 'V35', 'VII2', 'III56', 'I34'],\n                ['IV7', 'III6', 'VII2', 'III56', 'I34'],\n                ['IV7', 'I46', 'VII2', 'III56', 'I34'],\n\n\n                ['V7', 'VI35', 'V56', 'I35', 'VI6'],\n                ['V7', 'IV6', 'V56', 'I35', 'VI6'],\n                ['V7', 'II46', 'V56', 'I35', 'VI6'],\n\n                ['V7', 'VI35', 'V56', 'I35', 'VI35'],\n                ['V7', 'IV6', 'V56', 'I35', 'VI35'],\n                ['V7', 'II46', 'V56', 'I35', 'VI35'],\n\n                ['V7', 'VI35', 'V56', 'I2', 'VI35'],\n                ['V7', 'IV6', 'V56', 'I2', 'VI35'],\n                ['V7', 'II46', 'V56', 'I2', 'VI35'],\n\n                ['V7', 'VI35', 'III34', 'VI35', 'II6'],\n                ['V7', 'IV6', 'III34', 'VI35', 'II6'],\n                ['V7', 'II46', 'III34', 'VI35', 'II6'],\n\n                ['V7', 'VI35', 'III34', 'I35', 'VI6'],\n                ['V7', 'IV6', 'III34', 'I35', 'VI6'],\n                ['V7', 'II46', 'III34', 'I35', 'VI6'],\n\n                ['V7', 'VI35', 'III34', 'IV6', 'III6'],\n                ['V7', 'IV6', 'III34', 'IV6', 'III6'],\n                ['V7', 'II46', 'III34', 'IV6', 'III6'],\n\n                ['V7', 'VI35', 'III34', 'I2', 'II34'],\n                ['V7', 'IV6', 'III34', 'I2', 'II34'],\n                ['V7', 'II46', 'III34', 'I2', 'II34'],\n\n                ['V7', 'VI35', 'I2', 'IV56', 'III6'],\n                ['V7', 'IV6', 'I2', 'IV56', 'III6'],\n                ['V7', 'II46', 'I2', 'IV56', 'III6'],\n\n                ['V7', 'VI35', 'I2', 'II34', 'V35'],\n                ['V7', 'IV6', 'I2', 'II34', 'V35'],\n                ['V7', 'II46', 'I2', 'II34', 'V35'],\n\n                ['V7', 'VI35', 'I2', 'IV56', 'II34'],\n                ['V7', 'IV6', 'I2', 'IV56', 'II34'],\n                ['V7', 'II46', 'I2', 'IV56', 'II34'],\n\n\n                ['VI7', 'VII35', 'VI56', 'II35', 'V35'],\n                ['VI7', 'V6', 'VI56', 'II35', 'V35'],\n                ['VI7', 'III46', 'VI56', 'II35', 'V35'],\n\n                ['VI7', 'VII35', 'VI56', 'IV34', 'V6'],\n                ['VI7', 'V6', 'VI56', 'IV34', 'V6'],\n                ['VI7', 'III46', 'VI56', 'IV34', 'V6'],\n\n                ['VI7', 'VII35', 'VI56', 'II2', 'V6'],\n                ['VI7', 'V6', 'VI56', 'II2', 'V6'],\n                ['VI7', 'III46', 'VI56', 'II2', 'V6'],\n\n                ['VI7', 'VII35', 'IV34', 'V56', 'I35'],\n                ['VI7', 'V6', 'IV34', 'V56', 'I35'],\n                ['VI7', 'III46', 'IV34', 'V56', 'I35'],\n\n                ['VI7', 'VII35', 'IV34', 'II35', 'V6'],\n                ['VI7', 'V6', 'IV34', 'II35', 'V6'],\n                ['VI7', 'III46', 'IV34', 'II35', 'V6'],\n\n                ['VI7', 'VII35', 'IV34', 'V6', 'IV6'],\n                ['VI7', 'V6', 'IV34', 'V6', 'IV6'],\n                ['VI7', 'III46', 'IV34', 'V6', 'IV6'],\n\n                ['VI7', 'VII35', 'IV34', 'II2', 'III34'],\n                ['VI7', 'V6', 'IV34', 'II2', 'III34'],\n                ['VI7', 'III46', 'IV34', 'II2', 'III34'],\n\n                ['VI7', 'VII35', 'II2', 'V6', 'IV6'],\n                ['VI7', 'V6', 'II2', 'V6', 'IV6'],\n                ['VI7', 'III46', 'II2', 'V6', 'IV6'],\n\n                ['VI7', 'VII35', 'II2', 'III34', 'I2'],\n                ['VI7', 'V6', 'II2', 'III34', 'I2'],\n                ['VI7', 'III46', 'II2', 'III34', 'I2'],\n\n                ['VI7', 'VII35', 'II2', 'V56', 'III34'],\n                ['VI7', 'V6', 'II2', 'V56', 'III34'],\n                ['VI7', 'III46', 'II2', 'V56', 'III34'],\n\n\n                ['VII7', 'I35', 'VII56', 'III35', 'I6'],\n                ['VII7', 'VI6', 'VII56', 'III35', 'I6'],\n                ['VII7', 'IV46', 'VII56', 'III35', 'I6'],\n    \n                ['VII7', 'I35', 'VII56', 'I6', 'II35'],\n                ['VII7', 'VI6', 'VII56', 'I6', 'II35'],\n                ['VII7', 'IV46', 'VII56', 'I6', 'II35'],\n\n                ['VII7', 'I35', 'VII56', 'V34', 'I35'],\n                ['VII7', 'VI6', 'VII56', 'V34', 'I35'],\n                ['VII7', 'IV46', 'VII56', 'V34', 'I35'],\n\n                ['VII7', 'I35', 'V34', 'I35', 'IV6'],\n                ['VII7', 'VI6', 'V34', 'I35', 'IV6'],\n                ['VII7', 'IV46', 'V34', 'I35', 'IV6'],\n\n                ['VII7', 'I35', 'V34', 'III35', 'I6'],\n                ['VII7', 'VI6', 'V34', 'III35', 'I6'],\n                ['VII7', 'IV46', 'V34', 'III35', 'I6'],\n\n                ['VII7', 'I35', 'V34', 'VI6', 'V6'],\n                ['VII7', 'VI6', 'V34', 'VI6', 'V6'],\n                ['VII7', 'IV46', 'V34', 'VI6', 'V6'],\n\n                ['VII7', 'I35', 'V34', 'III2', 'IV34'],\n                ['VII7', 'VI6', 'V34', 'III2', 'IV34'],\n                ['VII7', 'IV46', 'V34', 'III2', 'IV34'],\n\n                ['VII7', 'I35', 'III2', 'VI6', 'V6'],\n                ['VII7', 'VI6', 'III2', 'VI6', 'V6'],\n                ['VII7', 'IV46', 'III2', 'VI6', 'V6'],\n\n                ['VII7', 'I35', 'III2', 'IV34', 'II2'],\n                ['VII7', 'VI6', 'III2', 'IV34', 'II2'],\n                ['VII7', 'IV46', 'III2', 'IV34', 'II2'],\n\n                ['VII7', 'I35', 'III2', 'VI56', 'IV34'],\n                ['VII7', 'VI6', 'III2', 'VI56', 'IV34'],\n                ['VII7', 'IV46', 'III2', 'VI56', 'IV34']\n]\ndigi_sequences = []\nfor sequence in sequences:\n    digi_sequence = [key for chord in sequence for key in chords_dictionary if chord == chords_dictionary[key]]\n    digi_sequences.append(digi_sequence)\n\n#### Matrix definition\nlist1 = [0, 1, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist2 = [35, 48, 12, 49, 13, 26, 14, 27, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist3 = [0, 2, 0, 0, 3, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist4 = [0, 0, 0, 0, 15, 0, 0, 16, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist5 = [0, 0, 0, 49, 13, 26, 14, 27, 40, 28, 41, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist6 = [0, 0, 0, 0, 16, 0, 0, 17, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist7 = [0, 0, 0, 0, 0, 0, 0, 29, 0, 0, 30, 0, 0, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist8 = [0, 0, 0, 0, 0, 0, 14, 27, 40, 28, 41, 5, 42, 6, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist9 = [0, 0, 0, 0, 0, 0, 0, 30, 0, 0, 31, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist10 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 43, 0, 0, 44, 0, 0, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist11 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 41, 5, 42, 6, 19, 7, 20, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist12 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 44, 0, 0, 45, 0, 0, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist13 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 9, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0]\nlist14 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 6, 19, 7, 20, 33, 21, 34, 47, 0, 0, 0, 0, 0, 0, 0]\nlist15 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 10, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0]\nlist16 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 23, 0, 0, 24, 0, 0, 0, 0, 0]\nlist17 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 20, 33, 21, 34, 47, 35, 48, 12, 0, 0, 0, 0]\nlist18 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 24, 0, 0, 25, 0, 0, 0, 0, 0]\nlist19 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 0, 0, 37, 0, 0, 38, 0, 0]\nlist20 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 34, 47, 35, 48, 12, 49, 13, 26, 0]\nlist21 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 0, 0, 38, 0, 0, 39, 0, 0]\nlist22 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist23 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist24 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist25 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist26 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist27 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlist28 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ngraph = torch.tensor([list1, list2, list3, list4, list5, list6, list7, list8, list9, list10, \n                      list11, list12, list13, list14, list15, list16, list17, list18, list19, list20,\n                      list21, list22, list23, list24, list25, list26, list27, list28], dtype=torch.int32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Conversion\n#### Retrieve indexes\n# Function that accounts for the sequence of appearence - you get 5 maps for one sequence\ndef get_sequence_indexes(seq, graph):\n    idx_dict = {}\n    indexes = []\n    chords_list = [] \n    for chord in seq:                           # for each chord in a sequence\n        chord_positions = []\n        for row in range(len(graph)):           # iterate through 28 rows of the matrix\n            val = (graph[row]==chord).nonzero() # find the indices of an array, where a condition is True\n            if len(val) > 0:                    # if the tensor is not empty, it is returning the position of the chord within the matrix\n                position_dict = {\n                    \"position\": val.item(),    # position in a row where the value should be inserted\n                    \"chord\": chord,            # value to be inserted\n                    \"row\": row                 # rows where the value should be inserted\n                    }\n                chord_positions.append(position_dict)\n                chords_list.append(position_dict)      \n    return chords_list\nchords_list = get_sequence_indexes(digi_sequences[0], graph)\nchords_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Group dicts\n# Group dicts\ngrouped_chord_sequences = []\nfor sequence in digi_sequences:             # for each harmonic sequence\n    dictionaries = get_sequence_indexes(sequence, graph)  # get positions where the chords should be inserted   \n    \n    grouped_chord_sequence = []\n    group = []\n    for i, dictionary in enumerate(dictionaries):\n        if i == 0:\n            group.append(dictionary)\n        else:\n            if dictionary.get('chord') == dictionaries[i-1].get('chord'):\n                group.append(dictionary)\n            else:\n                grouped_chord_sequence.append(group)\n                group = []\n                group.append(dictionary)\n    grouped_chord_sequence.append(group)\n    grouped_chord_sequences.append(grouped_chord_sequence)\n    \nlen(grouped_chord_sequences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Get matrices\ndef get_tensor_with_positions(list_entry):\n    zero_tensor = torch.zeros(28, 28)                # initialise a 2D tensor filled with zero values      \n    for dict_entry in list_entry: \n        zero_tensor[dict_entry.get('row')][dict_entry.get('position')] = dict_entry.get('chord') # insert value in a 2D tensor primarely filled with zeros\n    return zero_tensor\n\ndef get_sequence_data(grouped_dictionary):\n    sequence_data = []                                    # list of one sequence of 5 maps, should be zeroed at each itertion\n    for list_entry in grouped_dictionary:                 # for each dictionary in a list of grouped dicts\n        zero_tensor = get_tensor_with_positions(list_entry)\n        deep_copy = copy.deepcopy(zero_tensor)\n        sequence_data.append(deep_copy)\n    return sequence_data\n\nsequences_data = [] # list of sequences\nfor grouped_dictionary in grouped_chord_sequences:\n    sequence_data = get_sequence_data(grouped_dictionary)\n    sequences_data.append(sequence_data)\nlen(sequences_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Normalize and augment\nnormalized_sequences_data = []\nfor i in range(10):\n    num = float(\"49.0\" + str(i))\n    for sequence in sequences_data:\n        normalized_sequence_data = [] # holds 5 maps for 5 chords\n        for j in range(len(sequence)):\n            one_matrix = copy.deepcopy(sequence[j])\n            one_matrix /= 49\n            normalized_sequence_data.append(one_matrix)\n        normalized_sequences_data.append(normalized_sequence_data)\n        \nprint(len(normalized_sequences_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Reshape\n# Function to reshape sequence data\nreshaped_sequences = []\nfor i in range(0, (len(normalized_sequences_data)), 1):\n    channel_redim_sequence_2 = np.append(normalized_sequences_data[i][0], normalized_sequences_data[i][1])\n    channel_redim_sequence_3 = np.append(channel_redim_sequence_2, normalized_sequences_data[i][2])\n    channel_redim_sequence_4 = np.append(channel_redim_sequence_3, normalized_sequences_data[i][3])\n    channel_redim_sequence_5 = np.append(channel_redim_sequence_4, normalized_sequences_data[i][4])\n    channel_reshape_5 = channel_redim_sequence_5.reshape(5, 28, 28)\n    reshaped_sequences.append(channel_reshape_5)\nreshaped_sequences_np = np.array(reshaped_sequences)\n\nprint(reshaped_sequences_np.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle custom data\nnp.random.shuffle(reshaped_sequences_np)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Utils\ndef collate(batch):\n    batch = torch.tensor(batch).unsqueeze(1)\n    batch = batch.to(device)  \n    return batch[:,:,0:4], batch[:,:,4]\n\ndef reset_weights(m):\n    m.to(device)\n    for layer in m.children():\n        if hasattr(layer, 'reset_parameters'):\n            print(f'Reset trainable parameters of layer = {layer}')\n            layer.reset_parameters()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ConvLSTM cell and layer\n# Original ConvLSTM cell as proposed by Shi et al.\nclass ConvLSTMCell(nn.Module):\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n        super(ConvLSTMCell, self).__init__()\n        if activation == \"tanh\":\n            self.activation = torch.tanh \n        elif activation == \"relu\":\n            self.activation = torch.relu\n        \n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        self.conv = nn.Conv2d(\n            in_channels=in_channels + out_channels, \n            out_channels=4 * out_channels, \n            kernel_size=kernel_size, \n            padding=padding)           \n\n        # Initialize weights for Hadamard Products - changed to torch.rand\n        self.W_ci = nn.Parameter(torch.randn(out_channels, *frame_size)) # out-channels=28\n        self.W_co = nn.Parameter(torch.randn(out_channels, *frame_size)) # frame_size=(28, 28)\n        self.W_cf = nn.Parameter(torch.randn(out_channels, *frame_size))\n\n    def forward(self, X, H_prev, C_prev):\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n\n        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n\n        # Current Cell output\n        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n\n        # Current Hidden State\n        H = output_gate * self.activation(C)\n        return H, C\n\n### ConvLSTM layer\nclass ConvLSTM(nn.Module):\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n\n        super(ConvLSTM, self).__init__()\n        self.out_channels = out_channels\n\n        # We will unroll this over time steps\n        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n        kernel_size, padding, activation, frame_size)\n\n    def forward(self, X):\n        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n        # Get the dimensions\n        batch_size, _, seq_len, height, width = X.size()\n\n        # Initialize output\n        output = torch.zeros(batch_size, self.out_channels, seq_len, \n        height, width, device=device)\n        \n        # Initialize Hidden State\n        H = torch.zeros(batch_size, self.out_channels, \n        height, width, device=device)\n\n        # Initialize Cell Input\n        C = torch.zeros(batch_size,self.out_channels, \n        height, width, device=device)\n\n        # Unroll over time steps\n        for time_step in range(seq_len):\n            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n            output[:,:,time_step] = H\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ConvLSTM model 1 layer\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n    activation, frame_size):\n        super(Seq2Seq, self).__init__()\n        self.sequential = nn.Sequential()\n\n        # Add First layer (Different in_channels than the rest)\n        self.sequential.add_module(\n            \"convlstm1\", ConvLSTM(\n                in_channels=num_channels, out_channels=num_kernels,\n                kernel_size=kernel_size, padding=padding, \n                activation=activation, frame_size=frame_size)\n        )\n\n        self.sequential.add_module(\n            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n        ) \n\n        # Add Convolutional Layer to predict output frame\n        self.conv = nn.Conv2d(\n            in_channels=num_kernels, out_channels=num_channels,\n            kernel_size=kernel_size, padding=padding)\n\n    def forward(self, X):\n        # Forward propagation through all the layers\n        output = self.sequential(X)\n\n        # Return only the last output frame\n        output = self.conv(output[:,:,-1])        \n        return nn.Sigmoid()(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### K-fold Cross Validator\n# Params\ntorch.manual_seed(42)\nnum_epochs = 100\ncriterion = nn.BCELoss(reduction='sum')\n\n# Fold results storage objects\ntrain_start_results = {}\nval_start_results = {}\n\ntrain_end_results = {}\nval_end_results = {}\n\n# Per fold epoch results storage objects\ntrain_results_per_epoch = []\nval_results_per_epoch = []\n\ntrain_results = []\nval_results = []\n\naccuracy_train_per_epoch = []\naccuracy_val_per_epoch = []\n\nraw_accuracy_train_per_batch = []\nraw_accuracy_val_per_batch = []\n\naccuracy_train_results = []\naccuracy_val_results = []\n\nraw_accuracy_train_results = []\nraw_accuracy_val_results = []\n\n# Define the K-fold Cross Validator\nk_folds = 5\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Whole dataset\ndataset = reshaped_sequences_np\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n    print(f'FOLD {fold}')\n    \n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    val_subsampler = SubsetRandomSampler(val_ids)\n    \n    # Define data loaders for training and testing data in this fold\n    train_loader = DataLoader(dataset, batch_size=15, collate_fn=collate, sampler=train_subsampler)\n    val_loader = DataLoader(dataset, batch_size=15, collate_fn=collate, sampler=val_subsampler)\n    \n    # Initialization\n    model = Seq2Seq(num_channels=1, num_kernels=28, kernel_size=(3, 3), padding=(1, 1), activation=\"tanh\", frame_size=(28, 28)).to(device)\n    model.apply(reset_weights)  \n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    \n    train_results_per_epoch = []\n    val_results_per_epoch = []\n    for epoch in range(1, num_epochs+1):\n        train_loss = 0\n        train_accuracy = 0\n        model = model.to(device)\n        model.train()        \n        for batch, (x, y) in enumerate(train_loader, 1):\n            # Make sure input has no NaN\n            #input_data = torch.nan_to_num(x)\n            \n            # Move data and the model to cuda\n            #input_data = input_data.to(device)\n            x = x.to(device)\n            y = y.to(device)\n            \n            # Get the model prediction\n            #output = model(input_data)\n            output = model(x)\n            \n            # Make sure output has no NaN\n            #preds = torch.nan_to_num(output)\n            \n            # Move predictions to cuda\n            #preds = preds.to(device)\n            output = output.to(device)\n            \n            # Calculate train loss for each batch            \n            #loss_train = criterion(preds.flatten(), y.flatten())\n            loss_train = criterion(output.flatten(), y.flatten())\n            \n            # Backpropagate the loss\n            loss_train.backward()\n            \n            # Apply oplimizer and reset it\n            optimizer.step()                                               \n            optimizer.zero_grad() \n            \n            # Sum up train loss for all the batches in the epoch\n            train_loss += loss_train.item()\n            \n            # Accuracy\n            target_keys_train = get_keys(y)\n            pred_keys_train = get_keys(output)\n            \n            accuracy_train = get_accuracy(target_keys_train, pred_keys_train)\n            train_accuracy += accuracy_train\n            \n        # Calculate total train loss for the epoch\n        total_train_loss = train_loss / len(train_loader.dataset)\n        train_results_per_epoch.append(total_train_loss)\n        \n        # Calculate total train accuracy for the epoch\n        raw_accuracy_train_per_batch.append(accuracy_train)\n        if train_accuracy > 0:\n            total_train_accuracy = train_accuracy / len(train_loader.dataset)\n            accuracy_train_per_epoch.append(total_train_accuracy)            \n        else:\n            accuracy_train_per_epoch.append(train_accuracy)\n        \n        val_loss = 0\n        val_accuracy = 0\n        model.eval()                                                   \n        with torch.no_grad():                                          \n            for x, y in val_loader:\n                # Make sure input has no NaN\n                #input_data = torch.nan_to_num(x)\n                \n                # Move data to cuda\n                #input_data = input_data.to(device)\n                x = x.to(device)\n                y = y.to(device)\n                \n                # Get the model prediction\n                #output = model(input_data)\n                output = model(x)\n                \n                # Make sure output has no NaN\n                #preds = torch.nan_to_num(output)\n                \n                # Move preds to cuda\n                output = output.to(device)\n                \n                # Calculate validation loss for each batch \n                #loss_val = criterion(preds.flatten(), y.flatten())\n                loss_val = criterion(output.flatten(), y.flatten())\n                \n                # Sum up validation loss for all the batches in the epoch\n                val_loss += loss_val.item()\n                \n                # Accuracy\n                target_keys_val = get_keys(y)\n                pred_keys_val = get_keys(output)\n                \n                accuracy_val = get_accuracy(target_keys_val,pred_keys_val)\n                val_accuracy += accuracy_val\n                \n        # Calculate total validation loss for the epoch\n        total_val_loss = val_loss / len(val_loader.dataset)\n        val_results_per_epoch.append(total_val_loss)\n        \n        # Calculate total validation accuracy for the epoch\n        raw_accuracy_val_per_batch.append(val_accuracy)\n        if val_accuracy > 0:\n            total_val_accuracy = val_accuracy / len(val_loader.dataset)\n            accuracy_val_per_epoch.append(total_val_accuracy)\n        else:\n            accuracy_val_per_epoch.append(val_accuracy)\n        \n        # Store scores        \n        if epoch == 1:\n            val_start_results[fold] = total_val_loss\n            train_start_results[fold] = total_train_loss\n        else:\n            val_end_results[fold] = total_val_loss\n            train_end_results[fold] = total_train_loss\n            \n        print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n            epoch, total_train_loss, total_val_loss))\n    train_results.append(train_results_per_epoch)\n    val_results.append(val_results_per_epoch)\n    \n    accuracy_train_results.append(accuracy_train_per_epoch)\n    accuracy_val_results.append(accuracy_val_per_epoch)\n    raw_accuracy_train_results.append(raw_accuracy_train_per_batch)\n    raw_accuracy_val_results.append(raw_accuracy_val_per_batch)\n    \n    # Saving the model\n    save_path = f'./convlstm-custom2000-model-fold-{fold}.pth'\n    torch.save(model.state_dict(), save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stop CO2 tracker and print emissions\n\nemissions: float = tracker.stop()\nprint(f\"Emissions: {emissions} kg\")\n\n# Calculate the time spent\nstop_time = datetime.now() - start_time\ntime_spend = start_time - stop_time\n\n# Time logs\nexperiment.log_metric(\"start_time\", start_time) \nexperiment.log_metric(\"stop_time\", stop_time)\nexperiment.log_metric(\"time_spend\", time_spend)\n\n# Turn off Comet\nexperiment.end()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference\ndata_loader = DataLoader(dataset, batch_size=1, collate_fn=collate, drop_last=True)\ndata, target = next(iter(data_loader))\n\nmodel.eval()                                                   \nwith torch.no_grad():                                          \n    output = model(data)\noutput.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generated image visualization\nimg_gen = output[0].cpu().reshape(28, 28, 1)\nplt.imshow(img_gen)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target image visualization\ntarget_reshaped = target[0].cpu().reshape(28, 28, 1)\nplt.imshow(target_reshaped)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference for 15 sequences\ndata_loader = DataLoader(dataset, batch_size=15, collate_fn=collate, drop_last=True)\ndata, target = next(iter(data_loader))\n\nmodel.eval()                                                   \nwith torch.no_grad():                                          \n    output = model(data)\n\n# Reshape targets and generated\ntargets = target.reshape(15, 28, 28, 1)\nimgs_gen = output.reshape(15, 28, 28, 1)\n\n# Join tensors for a singe image\ncombined = torch.cat((targets, imgs_gen), 0)\ncombined.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Single normalization\n\nfig = plt.figure(figsize=(15, 15))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(2, 15),  # creates 2x2 grid of axes\n                 axes_pad=0.05,  # pad between axes in inch.\n                 )\n\nfor ax, im in zip(grid, combined.cpu()):\n    # Iterating over the grid returns the Axes.\n    ax.imshow(im)\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print start fold results\nprint(f'Start K-FOLD RESULTS FOR {k_folds} FOLDS')\nsum = 0.0\nfor key, value in train_start_results.items():\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average train: {sum/len(train_start_results.items())}')\n\nsum = 0.0\nfor key, value in val_start_results.items():\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average val: {sum/len(val_start_results.items())}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print final fold results\nprint(f'End K-FOLD RESULTS FOR {k_folds} FOLDS')\nsum = 0.0\nfor key, value in train_end_results.items():\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average train: {sum/len(train_end_results.items())}')\n\nsum = 0.0\nfor key, value in val_end_results.items():\n    print(f'Fold {key}: {value}')\n    sum += value\nprint(f'Average val: {sum/len(val_end_results.items())}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and validation results\n\nimport matplotlib.pyplot as plt\nx = list(range(0, 100))\n\nfig, ax = plt.subplots()\nt1, = ax.plot(x, train_results[0], c=\"blue\")\nt2, = ax.plot(x, train_results[1], c=\"brown\")\nt3, = ax.plot(x, train_results[2], c=\"green\")\nt4, = ax.plot(x, train_results[3], c=\"orange\")\nt5, = ax.plot(x, train_results[4], c=\"magenta\")\nv1, = ax.plot(x, val_results[0], c=\"blue\", ls=\"dashed\")\nv2, = ax.plot(x, val_results[1], c=\"brown\", ls=\"dashed\")\nv3, = ax.plot(x, val_results[2], c=\"green\", ls=\"dashed\")\nv4, = ax.plot(x, val_results[3], c=\"orange\", ls=\"dashed\")\nv5, = ax.plot(x, val_results[4], c=\"magenta\", ls=\"dashed\")\nax.legend((t1, t2, t3, t4, t5, v1, v2, v3, v4, v5), ('1st train fold', '2nd train fold', \"3rd train fold\", \"4th train fold\", \"5th train fold\", '1st val fold', '2nd val fold', \"3rd val fold\", \"4th val fold\", \"5th val fold\"), loc='upper right', shadow=True)\nax.set_xlabel('epochs')\nax.set_ylabel('loss')\nax.set_title('Train and validation results for 5 folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\nfig, ax = plt.subplots()\nt1, = ax.plot(x, train_results[0], c=\"blue\")\nt2, = ax.plot(x, train_results[1], c=\"brown\")\nt3, = ax.plot(x, train_results[2], c=\"green\")\nt4, = ax.plot(x, train_results[3], c=\"orange\")\nt5, = ax.plot(x, train_results[4], c=\"magenta\")\nax.legend((t1, t2, t3, t4, t5), ('1st train fold', '2nd train fold', \"3rd train fold\", \"4th train fold\", \"5th train fold\"), loc='upper right', shadow=True)\nax.set_xlabel('epochs')\nax.set_ylabel('loss')\nax.set_title('Train results for 5 folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\nfig, ax = plt.subplots()\nv1, = ax.plot(x, val_results[0], c=\"blue\", ls=\"dashed\")\nv2, = ax.plot(x, val_results[1], c=\"brown\", ls=\"dashed\")\nv3, = ax.plot(x, val_results[2], c=\"green\", ls=\"dashed\")\nv4, = ax.plot(x, val_results[3], c=\"orange\", ls=\"dashed\")\nv5, = ax.plot(x, val_results[4], c=\"magenta\", ls=\"dashed\")\nax.legend((v1, v2, v3, v4, v5), (\"1st val fold\", \"2nd val fold\", \"3rd val fold\", \"4th val fold\", \"5th val fold\"), loc='upper right', shadow=True)\nax.set_xlabel('epochs')\nax.set_ylabel('loss')\nax.set_title('Validation results for 5 folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open(\"./\" + \"train_results.json\", 'w') as outfile:\n    json.dump(train_results, outfile)\nwith open(\"./\" + \"val_results.json\", 'w') as outfile:\n    json.dump(val_results, outfile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}